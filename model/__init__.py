# é¦–å…ˆå°†jsonæ ¼å¼æ•°æ®è½¬æ¢æˆæ ‡æ³¨æ ¼å¼jsonlæ ¼å¼æ–‡ä»¶ï¼ˆæ­¤å¤„è½¬æ¢éœ€è¦å°†jsonæ ¼å¼ä¸­çš„keyå€¼è®¾ç½®æˆç›¸åº”çš„keyå€¼ï¼‰ï¼Œç„¶åå°†jsonlæ ¼å¼çš„æ–‡ä»¶è½¬æ¢æˆtokenizeræ ¼å¼æ–‡ä»¶
# jsonæ–‡ä»¶ä¸‹è½½
# https://huggingface.co/datasets/whalning/Chinese-medical-QA

import yaml
from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer, \
    DataCollatorForLanguageModeling
from .dataset_loader import load_tokenized_dataset
import torch


class LORASFTtrainModel(object):
    def __init__(self, sftcfg, loracfg):
        # === Load Configs ===
        with open(sftcfg, "r", encoding="utf-8") as f:
            self.sft_cfg = yaml.safe_load(f)
        with open(loracfg, "r", encoding="utf-8") as f:
            self.lora_cfg = yaml.safe_load(f)

        # === Load Model + LoRA ===
        self.model = AutoModelForCausalLM.from_pretrained(self.sft_cfg["model_name_or_path"], device_map="auto")
        self.tokenizer = AutoTokenizer.from_pretrained(self.sft_cfg["model_name_or_path"], trust_remote_code=True)
        self.peft_config = LoraConfig(**self.lora_cfg)
        self.model = get_peft_model(self.model, self.peft_config)

    def Update(self):
        # === Load Dataset ===
        train_dataset = load_tokenized_dataset(self.sft_cfg["model_name_or_path"], self.sft_cfg["train_data_path"])
        eval_dataset = load_tokenized_dataset(self.sft_cfg["model_name_or_path"], self.sft_cfg["eval_data_path"])
        data_collator = DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=False)
        # === Training Args ===
        args = TrainingArguments(
            output_dir=self.sft_cfg["output_dir"],
            per_device_train_batch_size=int(self.sft_cfg["per_device_train_batch_size"]),
            num_train_epochs=int(self.sft_cfg["num_train_epochs"]),
            learning_rate=float(self.sft_cfg["learning_rate"]),
            logging_steps=int(self.sft_cfg["logging_steps"]),
            save_strategy=self.sft_cfg["save_strategy"],
            report_to=["tensorboard"],
        )
        trainer = Trainer(
            model=self.model,
            args=args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
        )
        trainer.train()
        self.model.save_pretrained(self.sft_cfg["output_dir"])


class LORASFTinferenceModel(object):
    def __init__(self, model_dir):
        # 1ï¸âƒ£ åŠ è½½ tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
        # 2ï¸âƒ£ åŠ è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨é€‰æ‹© GPU/CPUï¼Œä½¿ç”¨åŠç²¾åº¦åŠ é€Ÿï¼‰
        self.model = AutoModelForCausalLM.from_pretrained(model_dir,
                                                          torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
                                                          device_map="auto", trust_remote_code=True)
        self.model.eval()

    def infernece(self, prompt):
        # inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)#ä¼šæŠŠè¾“å…¥ä¹Ÿè§£ç å‡ºæ¥
        # 3ï¸âƒ£ æ„é€ è¾“å…¥ï¼ˆQwen3 æ¨èä½¿ç”¨ apply_chat_templateï¼‰
        # å¦‚æœæ˜¯å¤šè½®å¯¹è¯ï¼Œå¯ä¼ å…¥ messages=[{"role":"user","content":prompt}]
        inputs = self.tokenizer.apply_chat_template(
            [{"role": "user", "content": prompt}],
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt").to(self.model.device)
        attention_mask = (inputs != self.tokenizer.pad_token_id).to(self.model.device)
        # 4ï¸âƒ£ ç”Ÿæˆè¾“å‡ºï¼ˆåŠ ä¸Š temperatureã€top_p æ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§ï¼‰
        outputs = self.model.generate(inputs, attention_mask=attention_mask, max_new_tokens=1024,
                                      temperature=0.7, top_p=0.9, do_sample=True,
                                      pad_token_id=self.tokenizer.eos_token_id)
        # 5ï¸âƒ£ è§£ç è¾“å‡ºï¼ˆå–ç”Ÿæˆçš„æ–°å†…å®¹éƒ¨åˆ†ï¼‰
        response = self.tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)
        print("ğŸ©º æ¨¡å‹å›å¤ï¼š", response)
        return response

